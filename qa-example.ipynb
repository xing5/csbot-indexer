{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fe55dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5330b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b8def38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "\n",
    "client = weaviate.Client(url=\"http://localhost:8080/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40a41646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'sk-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6dcc1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Weaviate(client, \"DocumentChunk\", \"chunk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8cf4c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"how to set up the docker compose yaml\"\n",
    "docs = vectorstore.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29886417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\" to throughout the documentation as  configuration yaml files . If self-hosting, we recommend starting with Docker and gaining familiarity with Weaviate and its configurations. You can later apply this knowledge when you are creating your Helm charts. Run an unreleased version\\u200b You can run Weaviate with  docker-compose , building your own container off the  master  branch. Note that this may not be an officially release, and may contain bugs. git clone https://github.com/weaviate/weaviate.git cd weaviate docker build --target weaviate -t name-of-your-weaviate-image . Then, make a  docker-compose.yml  file with this new image. For example: version: '3.4' services: weaviate: image: name-of-your-weaviate-image ports: - 8080:8080 environment: CONTEXTIONARY_URL: contextionary:9999 QUERY_DEFAULTS_LIMIT: 25 AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true' PERSISTENCE_DATA_PATH: './data' E\", metadata={}),\n",
       " Document(page_content=\"Docker Compose Configurator\\u200b You can use the configuration tool below to customize your Weaviate setup for your desired runtime. Example configurations\\u200b If you are new to Docker (Compose) and containerization, check out our Docker Introduction for Weaviate Users. To start Weaviate with docker-compose, you need a docker-compose configuration file, typically called  docker-compose.yml . You can obtain it from the configuration tool above or alternatively pick one of the examples below. Additional environment variables can be set in this file, which control your Weaviate setup, authentication and authorization, module settings, and data storage settings. A comprehensive of list environment variables can be found on this page. Persistent volume\\u200b It's recommended to set a persistent volume to avoid data loss and improve reading and writing speeds. Add the following snippet to your Docker Compose YAML file: services: weaviate: volumes: - /var/weaviate:/var/lib/weaviate # etc Make sure to run  $ docker-compose down  when shutting down, this writes all the files from memory to disk. Weaviate without any modules\\u200b An example docker-compose setup\", metadata={}),\n",
       " Document(page_content=' module\\u200b text2vec-transformers module\\u200b An example docker-compose setup file with the transformers model  sentence-transformers/msmarco-distilroberta-base-v2  is: version: \\'3.4\\' services: weaviate: image: semitechnologies/weaviate:1.18.3 restart: on-failure:0 ports: - \"8080:8080\" environment: QUERY_DEFAULTS_LIMIT: 20 AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: \\'true\\' PERSISTENCE_DATA_PATH: \"./data\" DEFAULT_VECTORIZER_MODULE: text2vec-transformers ENABLE_MODULES: text2vec-transformers TRANSFORMERS_INFERENCE_API: http://t2v-transformers:8080 CLUSTER_HOSTNAME: \\'node1\\' t2v-transformers: image: semitechnologies/transformers-inference:sentence-transformers-msmarco-distilroberta-base-v2 environment: ENABLE', metadata={}),\n",
       " Document(page_content=' configuration file from the Weaviate helm chart: helm show values weaviate/weaviate > values.yaml Adjust the configuration in the values.yaml (Optional)\\u200b Note: You can skip this step and run with all default values. In any case,\\nmake sure that you set the correct Weaviate version. This may either be through\\nexplicitly setting it as part of the  values.yaml  or through overwriting the\\ndefault as outlined in the deploy step below. In the  values.yaml \\nfile you can tweak the configuration to align it with your\\nsetup. The yaml file is extensively documented to help you align the\\nconfiguration with your setup. Out of the box, the configuration file is setup for: 1 Weaviate replica. (This cannot be changed at the moment, see below) The  text2vec-contextionary module is enabled and running with 1 replica. (This can be adjusted based on the expected load). Other modules, such as  text2vec-transformers , qna-transformers or img2vec-neural are disabled by default. They can be enabled by setting the respective enabled flag to true .', metadata={})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e03082e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adb35cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nTo set up the docker compose yaml to use Weaviate locally, you will need to create a docker-compose.yml file with the following information:\\n\\nversion: '3.4'\\nservices:\\n  weaviate:\\n    image: name-of-your-weaviate-image\\n    ports:\\n      - 8080:8080\\n    environment:\\n      CONTEXTIONARY_URL: contextionary:9999\\n      QUERY_DEFAULTS_LIMIT: 25\\n      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'\\n      PERSISTENCE_DATA_PATH: './data'\\n      DEFAULT_VECTORIZER_MODULE: text2vec-transformers\\n      ENABLE_MODULES: text2vec-transformers, qna-transformers, img2vec-neural\\n      TRANSFORMERS_INFERENCE_API: http://t2v-transformers:8080\\n      CLUSTER_HOSTNAME: 'node1'\\n    volumes:\\n      - /var/weaviate:/var/lib/weav\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = load_qa_chain(OpenAI(temperature=0), chain_type=\"refine\")\n",
    "query = \"how to set up the docker compose yaml to use weaviate locally\"\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48425ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
